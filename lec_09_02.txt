[p.07]

Okay, so let's talk about the conditions under which we can use p_θ(s_t) in place of p_{θ'}(s_t) and still have a reasonable objective that accurately approximates the return of the new policy.

[p.08]

So to recap, what we want to do is we want to ignore the distribution mismatch.
We want to use p_θ(s_t) instead of p_{θ'}(s_t) such that the only dependence on θ' in this whole equation is in the importance weight because we know from our previous policy gradient lecture that if we differentiate the thing on the right-hand side, we get exactly the policy gradient.
So we want this to be true so that J(θ') minus J(θ) is well approximated by bar{A}(θ') this thing on the right-hand side so that we could just maximize bar{A}.
And get a better new policy.
And what we're going to try to show is that p_θ(s_t) is close to p_{θ'}(s_t) when π_θ is close to π_{θ'}.
And when that's the case, then the right-hand side approximates the left-hand side, meaning the difference between them can be bounded by some quantity which is small if the difference between π_θ and π_{θ'} is small.

[p.09]

Okay, so this is what we're interested in.
We want to show that π_θ(s_t) is close to π_{θ'}(s_t).
Sorry, p_θ(s_t) is close to p_{θ'}(s_t) when π_θ is close to π_{θ'}.
So let's start with a simple case.
Let's first assume that π_θ is a deterministic policy, which means that we can express it as a_t equals π_θ(s_t).
We'll generalize this to stochastic policies later, but I think the deterministic derivation provides a little bit of intuition.
Which then makes the stochastic case easier to understand.
So in this case what we're going to try to show is that the state marginals for θ and θ' are close if π_{θ'} is close to π_θ.
Now π_{θ'} is not necessarily deterministic, and the way that we define close is that the probability that π_{θ'} assigns to any action that is not the action that π_θ would have taken, is less than or equal to ϵ.
Essentially there's a bounded probability that π_{θ'} does something different.
So if this is the case then we can write the state marginal at time step t for θ' as the sum of two terms.
The first term describes the case where every single time step up to t, the new policy π_{θ'} did exactly the same thing as π_θ.
And since the probability of doing exactly the same thing as 1 - ϵ, this term has a multiplier of (1 - ϵ)^t in front of it, and the state marginal is exactly p_θ(s_t), because if you did all the same things as π_θ, you have the same state distribution.
The other term is simply everything else.
It's 1 - (1 - ϵ)^t, and it multiplies some other state distribution, which we're going to assume that we don't know anything about.
So we're going to call it p_{mistake}.
This is the distribution over states you get if you made at least one mistake, and we're going to assume that we don't know anything about it.
So it could be like that tightrope walker from the imitation learning lecture at the beginning of class, where if you fall off the tightrope, you can never recover, and you're in some completely different place.
So, of course, most of you hopefully recognize this equation as being...
exactly the same as the equation we had before when we analyzed behavioral cloning.
And in fact, our assumption is very similar to the assumption we had before, which is that the new policy has a bounded probability of deviating from the old policy.
So this is the probability we made no mistake, and this is some other distribution on which we're going to make no assumption whatsoever.
So hopefully this seems familiar to all of you.
And just like before, this equation implies that we can write the total variation divergence between p_{θ'} and p_θ as essentially just the part that's different, right?
So that (1 - ϵ)^t times p_θ(s_t) part has zero total variation divergence against p_θ(s_t).
So it's only that second part, and that second part has a multiplier of (1 - (1 - ϵ)^t) in front of it, and that multiplies the TV(total variation) divergence between p_{mistake} and p_θ.
Now we can't bound this TV divergence to p_{mistake} - p_θ because we're making no assumption on p_{mistake}.
So the only bound we have is the trivial bound, which is that any total variation divergence always has to be less than or equal to 2.
So that means that the total variation divergence between the state marginals is bounded by 2(1 - (1 - ϵ)^t).
This is all exactly the same as in the imitation learning analysis that we had in the second lecture.
And we use the same useful identity, which is that (1 - ϵ)^t is greater than or equal to 1 - ϵt for any ϵ between 0 and 1, and that allows us to express this bound as a quantity that is linear in ϵ and linear in t.
So the difference between the state marginals is less than or equal to 2ϵt.
Okay.
So it's not a great bound, but it is a bound because it shows that as ϵ decreases, the state marginals become similar to one another.
But of course, this was all for the case of the deterministic policy p_θ.
In general, p_θ will not be deterministic.

[p.10]

So what we're going to do next is we're going to analyze the general case where π_θ is an arbitrary distribution.
And this proof follows the proof in the trust region policy optimization paper, which is a reference to the bottom of the slide.
Okay.
So here we're going to say that p_{θ'} is close to p_θ if their total variation divergence is bounded by ϵ for all states s_t.
Now, it turns out that we actually don't need the bound to be pointwise.
It turns out we can actually use a bound that's an expectation.
But let's go with a pointwise bound for now because it's a little easier to explain.
But keep in mind this will also hold true.
If the bound is in expectation, meaning the expected value of the total variation divergence is less than or equal to ϵ.
Okay.
So a useful lemma that we're going to use for this analysis is this one.
This lemma will take a little bit of unpacking.
And I'm not going to prove it.
This is in prior work, but it's referenced in the trust region policy optimization paper at the bottom.
So this lemma says that if you have two distributions, which I'm going to call p_X and p_Y, and the total variation divergence between these two distributions, meaning the sum over all values of x of the absolute value of the difference between p_X(x) and p_Y(x), is equal to ϵ.
So the total variation divergence between them is ϵ.
Then there exists a joint distribution p(x,y) so that its marginal p(x) is p_X(x) and its marginal p(y) is p_Y(y).
And the probability of x equals y is 1 - ϵ.
So to unpack this lemma a little bit, what this says is that if you have two distributions over the same variable, p_X(x) and p_Y(x), and their total variation divergence is ϵ, then you can construct a joint distribution over two variables, such that the marginals of that joint distribution with respect to its first and second argument are the two distributions you started with, and the probability of its two arguments being equal is 1 - ϵ.
And intuitively the reason that this lemma is useful to us is that we essentially want a generalization of the assumption from the previous slide.
So in the previous slide, our assumption was that there's a 1 - ϵ probability that π_{θ'} takes the same action as π_θ.
So if we could express a probability that π_{θ'} and π_θ will take the same action when both of them are stochastic, then we can use that to generalize the result from the previous slide to the case where the policies are stochastic.
So essentially this says that p_X(x) agrees with p_Y(y) with probability ϵ.
And that means that π_{θ'} takes a different action from π_θ with probability at most ϵ.
In retrospect, this is actually kind of obvious, because if their total variation divergence is ϵ, and total variation divergence is the difference in probability, it kind of makes sense that the sliver of probability mass that is different between them would have a volume of ϵ.
So if you're if you kind of have a more geometric way of thinking about it, just imagine the two distributions as bar graphs, overlay them on top of each other and look at the differences in the bars.
The volume of those differences will be equal to ϵ, which means that your probability of doing something different to ϵ.
So that's kind of the geometric intuition.
But if you prefer to think about things symbolically, then hopefully this lemma kind of puts your mind at ease.
That, in fact, if the total variation divergence is ϵ, then the probability of a mistake is at most ϵ.
So what this lemma allows us to do is it allows us to state the same result that we had on the previous slide, which is the total variation divergence between the state marginals is 1 - (1 - ϵ)^t times p_{mistake} - p_θ, only now we can say this even when π_θ is stochastic, provided the total variation divergence between π_{θ'} and π_θ is bounded by ϵ.
So from here, everything is exactly the same.
We can write the same bound, and we can say that the state marginals differ by at most 2ϵt.
So essentially the trick that we used on this slide was to use this lemma to express a probability that two policies will take different actions in terms of the total variation divergence.

[p.11]

So this is the result that we have on the state marginals.
So what does this tell us about the actual objective value?
What we want is we actually want to relate the two objectives expressed in terms of advantage values.
Well, so for this I'm going to derive another little calculation, which describes the expected values of functions under distributions when the total variation divergence between those distributions is bounded.
So we're going to have some function f(s_t), which in our case is this complicated thing that involves expectations over actions and advantage values, but it doesn't really matter what it is.
Whatever it is that's called f(s_t), we can bound its expected value between the two distributions.
So we can write the expected value under p_{θ'}(s_t) of f(s_t) as the sum over all possible states of p_{θ'} times f.
And we know that this quantity, is greater than or equal to the sum over all the states of p_θ times f minus the total variation divergence between p_θ and p_{θ'} times the maximum value that f could possibly take on.
So the way that I arrive at this calculation is I write p_{θ'} as being equal to p_{θ'} plus p_θ minus p_θ.
I group the terms to get a p_θ times f minus a p_θ minus p_{θ'} times f.
And while I don't know the absolute value of the difference in the probabilities, I only bound the total variation divergence, if I multiply them by the largest value that f takes, I'm being as pessimistic as possible.
So that's what allows me to write this inequality.
If this is not clear to you, then I would recommend pausing the lecture now, getting out a piece of paper, and actually deriving this inequality yourself.
It's a good exercise to do to make sure that you understand how to manipulate total variation divergence, so if this inequality doesn't make sense, please get out a piece of paper, try to work through it now.
So pause the lecture and do that.
And if you're having trouble doing that, then please ask a question in the comments, and we'll go over this in more detail in class.
Okay.
So then all we do is we take that first term, and we notice that it's just the definition of the expected value under p_θ, and we take the second term and we replace it by our bound on the total variation divergence, which means that the expected value under p_{θ'} of f is bounded below by it's expected value under p_θ, minus an error term which is 2ϵt, times the largest value that f can take on.
Now this error term might seem pretty big, because the largest value of f might be huge, but remember that everything is multiplied by ϵ.
So as the two policies get closer together, as ϵ gets small, that second term can always be made arbitrarily small so long as f is bounded, basically, so long as f doesn't go off to infinity for any state.
So this was the equation that we were originally concerned about.
We were concerned about the expected value under p_{θ'}(s_t) of the expected value over the actions of the importance sampled estimator for the advantage.
So by taking everything inside the brackets to be f, then we can bound this quantity below by the expected value under p_θ(s_t) of the same quantity, and this thing now just looks exactly like the thing that we differentiate to get the policy gradient, minus an error term, and the error term is 2ϵt times a constant C, and the constant C is the largest value that the thing inside the brackets, the thing inside the state expectation, can take on.
Take a moment to think about what the largest possible value for that quantity inside the brackets could be, basically.
What should we use for C?
The thing inside the brackets is a really complicated equation, but notice that most of the terms in that equation are probabilities, and we know that probabilities have to sum to 1.
So in fact, what we can notice is that the quantity inside the brackets is basically, it's some expected value, of an advantage.
And what is an advantage?
Well, an advantage is the sum of rewards over time.
So that means that the largest value that C could take on is the largest possible reward times the number of time steps, because all those importance weights and all those expectations, they have to sum up to 1.
So that means that the, basically, the expected value of any function can't possibly be larger than the largest value that function takes on, and the largest value that an advantage can take on is the number of time steps times the largest reward.
Because ultimately, an advantage is the sum of rewards over time.
So that means that the C is on the order of the number of time steps times r_{max}, and if you have infinite time steps, but you use a discount, then you know that the discount values, they form a geometric series, so they have to sum to 1 over 1 - γ.
So in a finite horizon case, C is capital T times r_{max}.
In an infinite horizon case, it's r_{max} over 1 - γ.
By the way, as an aside here, if you're doing reinforcement learning theory, and you ever see a term that looks like 1 over (1-γ), just mentally substitute the time horizon for that, because 1 over (1-γ) is essentially the equivalent of a horizon in the infinite horizon case.
It's basically the number of time steps, the effective number of time steps that you're summing rewards over.
Okay.
So essentially, all this says is the maximizing this equation at the bottom maximizes a bound on the thing that we really want, which is the expectation under p_{θ'}, which we've proven to be the same as maximizing the RL objective.
And the thing that we have to watch out for is that we get this error term that scales as 2ϵt times a constant.
Everything in that error term is a constant except for ϵ, and ϵ is the total variation divergence between your new policy and your old policy.
So take a moment at this point to think about what kind of RL algorithm we should use informed by this derivation.
Basically, this derivation suggests that a certain very tractable objective is a good approximation to the true RL objective under certain circumstances.
And this implies something about the sort of reinforcement learning algorithm that we should be using if we want to get good performance.

[p.12]

All right, so what we have so far is that maximizing this objective, basically the expected value under p_θ, the expected value under π_θ of the importance weighted advantage, is a good way to maximize the RL objective so long as π_{θ'} is close to π_θ in terms of total variation divergence.
Essentially, if you restrict θ' to not go too far from θ so this constraint is satisfied, then maximizing this tractable objective is the same as maximizing the true RL objective.
How do we maximize this objective?
Well, we take its derivative with respect to θ'.
And θ' only appears in the importance weight, so when we take its derivative, we get exactly the policy gradient.
So our derivation, what it has shown so far, is that this is a good thing to do if θ' stays close to θ.
So for small enough ϵ, this is guaranteed to improve J(θ') minus J(θ), which means that it's guaranteed to improve the RL objective.
Okay, so in the next part of the lecture, we'll talk about how to actually do this in practice.