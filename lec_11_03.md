![11_17](images/lec_11_17.png)

Alright, let's talk about how we can train uncertainly aware neural network models to serve as our uncertainty aware dynamics models for model-based RL.

![11_18](images/lec_11_18.png)

So how can we have uncertainty aware models?

Well, one very simple idea is to use the entropy of the output distribution.

I'll tell you right now this is a bad idea, this does not work, but I'm going to explain it just to make it clear why it doesn't work.

So let's say that you have your neural network dynamics model that takes in s and a as input and it produces p(s_{t+1}|s_t,a_t), which could be represented by a softmax distribution if you're in the discrete action setting, or it could be represented by a multivariate Gaussian distribution in the continuous setting.

So in the multivariate Gaussian case, you output a mean and a variance.

In the softmax case, you just output the logit for every possible next state.

Why is this not enough?

Well, we talked about how the problem we're having is this erroneous extrapolation.

So for the setting where we have limited data, we might overfit and make erroneous predictions, and the particular kind of errors that we're especially concerned with are ones where the optimizer can exploit those errors by optimizing against our model.

When the optimizer optimizes against our model, what it's really going to be doing is it's going to be finding out-of-distribution actions that lead to out-of-distribution states that then lead to more out-of-distribution states, which means that our model is going to be asked to make predictions for states and actions that it was not trained on.

The problem is that if the model is outputting the uncertainty and it's trained with a regular maximum likelihood, the uncertainty itself will also not be accurate for our out-of-distribution inputs.

So out-of-distribution inputs will result in erroneous predictions like an erroneous mean, but they'll also result in an erroneous variance for the same exact reason.

And this is because the uncertainty of the neural net output is the wrong kind of uncertainty.

So if you imagine this highly overfitted model, you could say, well, what variance is this model going to be predicted?

Let's say that the blue curve represents the predictions from the model, the model outputs a mean, and the variance over y at every point.

Well, if it looks at the training points, the training means are basically exactly the same as the actual values.

So the optimal variance for its output is actually zero.

This model will be extremely confident.

But of course, it's completely wrong.

And we'll see exactly the same thing from deep nets.

we'll see very confident predictions that are very good on the training points, but are both incorrect and overconfident on the test points.

And this is not something special about neural nets.

It's not about neural nets being bad at estimating uncertainty.

It's just because this is the wrong kind of uncertainty to be predicting.

This measure of entropy is not trying to predict the uncertainty about the model.

It's trying to predict how noisy the dynamics are.

See, there are two types of uncertainty.

And there are a variety of names that people have used for them, but we can call them aleatoric or statistical uncertainty, which is essentially the case where you have a function that is itself noisy.

And then we have epistemic or model uncertainty, which happens not because the true function itself is noisy or not, but because you don't know what the right function is.

And these are fundamentally different kinds of uncertainty.

Aleatoric uncertainty doesn't go down necessarily as you collect more data.

If the true function is noisy, no matter how much data you collect, you will have high entropy outputs just because the true function has high entropy.

For example, if you're learning a dynamics model for a game of chance, for a game where you roll two dice, the correct answer for the model that models the numerical value of the sum of those two dice is going to be random.

It's never going to become deterministic as you collect more data.

Seeing the dice roll more and more doesn't allow you to turn that statistic, that stochastic system into a deterministic one.

That's aleatoric uncertainty.

That's when the world itself is actually random.

Epistemic uncertainty comes from the fact that you don't know what the model is.

So epistemic uncertainty would be like the setting we had when approaching the cliff or walking around on the top of the mountain.

Once you collect enough data, that uncertainty goes away.

But in the limited data regime, you have to maintain that uncertainty because you don't know what the model actually is.

This is essentially the setting where the model is certain about the data, but we are not certain about the model.

And that's what we want.

Maximum likelihood training doesn't give you this.

So just outputting a distribution over the next state or a Gaussian distribution with a mean and variance will not get you this capability.

![11_19](images/lec_11_19.png)

So how can we get it?

Well, we can try to estimate model uncertainty.

And there are a number of different techniques for doing this.

So this is basically the setting where the model is certain about the data, but we are not certain about the model.

In order to not be certain about the model, we need to represent a distribution over models.

So before we have one neural net that outputted a distribution over s_{t+1}, and it has some parameters θ.

So being uncertain about the model really means being uncertain about θ.

So usually we would estimate θ as the argmax of the log probability of θ given our data set, which when we're doing maximum likelihood estimation, we take to also be the argmax of the log probability of the data given θ.

And that presumes having a uniform prior.

But can we instead estimate the full distribution p(θ|D)?

So instead of just finding the most likely θ, what if we actually try to estimate the full distribution (θ|D), and then use that to get our uncertainty?

That is the right kind of uncertainty to get in this situation.

So the entropy of this distribution will tell us the model uncertainty, and we can average out the parameters and get a posterior distribution over the next state.

So when we then have to predict, we would actually integrate out our parameters.

So instead of taking the most likely θ and outputting the p(s_{t+1}|s_t,a_t), and that most likely θ, we'll output our parameters by integrating out θ, by taking the ∫p(s_{t+1}|s_t,a_t,θ)p(θ|D)dθ.

Now of course, for large high-dimensional parameter spaces of the sort that we would have with neural nets, performing this operation exactly is completely intractable.

So we have to resort to a variety of different approximations, and that's what we're going to talk about in this lecture.

So intuitively, you could imagine this is producing some distribution over next states, which is going to integrate out all the uncertainty in your model.

![11_20](images/lec_11_20.png)

So one choice that we could use is something called a Bayesian neural network.

I'm not going to go into great detail about Bayesian neural networks in this lecture, because it requires a little bit of machinery, a little bit of variational inference machinery, which we're actually going to cover next week.

But I do want to explain the high-level idea behind Bayesian neural nets.

So in a standard neural net of the sort shown on the left, you have inputs X and outputs Y, and every weight, every connection between the hidden units, the inputs and the outputs, is just a number.

So all the neural nets that you've trained so far in this class basically work on this principle.

In Bayesian neural networks, there's a distribution over every weight.

In the most general case, there's actually a joint distribution over all the weights.

If you want to make a prediction, what you can do is you can sample from this distribution, essentially sample a neural net from the distribution over neural nets, and ask it for its prediction.

And if you want to get a posterior distribution over predictions, if you want to sample from the posterior distribution, you would sample a neural net and then sample a Y given that neural net.

And you could repeat this process multiple times, if you want to get many samples, to get a general impression of the true posterior distribution y given x, with θ having been integrated out.

Now, modeling full joint distributions over the parameters is very difficult, because the parameters are very high dimensional.

So there are a number of common approximations that could be made.

One approximation is to estimate the parameter posterior, this p(θ|D), as a product of independent marginals.

This basically means that every weight is distributed randomly, but independently of all the other weights.

This is of course not a very good approximation, because in reality the weights have very tightly interacting effects.

So, you know, if you want to vary one weight, and you vary the other one in the opposite direction, maybe your function doesn't change very much, but if you vary them independently, it could change quite a lot.

So using a product of independent marginals to estimate the parameter posterior is a very crude approximation, but it's a very simple and tractable one, and for that reason it is used quite often.

A common choice for the independent marginals is to represent each marginal with a Gaussian distribution.

And that means that for every weight, instead of learning its numerical value, you learn its mean value and its variance.

So for each weight you have not one number, but two numbers now.

You have the expected weight value, and the uncertainty about the weight.

And that is a very nice intuitive interpretation, because you've gone from learning just a single weight vector to learning a mean weight vector, and for every dimension you have a variance.

For more details about these kinds of methods, here are a few relatively simple papers on this topic.

Weight Uncertainty in Neural Networks by Blundell et al. and Concrete Dropout by Gall et al.

Although there are many more recent substantially better methods that you could actually use if you want to do this in practice.

So Bayesian neural networks are actually a reasonable choice to get an uncertainty-aware model.

To learn more about how to train them, check out these papers, or hang on until we cover the variational inference material next week.

![11_21](images/lec_11_21.png)

Today we're instead going to talk about a simpler method that from my experience actually works a little bit better in model-based reinforcement learning, and that's to use bootstrap ensembles.

Here is the basic idea behind bootstrap ensembles.

I'll present it first intuitively, and then discuss a little bit more mathematically what it's doing.

What if instead of training one neural network to give us the distribution over the next state given the current state in action, we instead train many different neural networks, and we somehow diversify them so that each of those neural networks learns a slightly different function?

Ideally, they would all do similar and accurate things on the training data, but they would all make different mistakes outside of the training data.

If we can train this kind of ensemble of models, then we can get them to essentially vote on what they think the next state will be, and the dispersion in their votes will give us an estimate of uncertainty.

Mathematically, this amounts to estimating your parameter posterior, p(θ|D), as a mixture of Dirac δ distributions.

So you've probably learned about mixtures of Gaussians.

A mixture of Dirac deltas is like a mixture of Gaussians, and instead of Gaussians, you have very narrow spikes, so each element has no variance.

It's just a mixture of δ functions, where each δ function is centered at the parameter vector for the corresponding network in the ensemble.

So intuitively, you can train multiple models and see if they agree as your measure of uncertainty.

Formally, you get a parameter posterior, p(θ|D), represented as this mixture of Dirac deltas, which means that if you want to integrate out your parameters, you simply average over your models.

So you construct a mixture distribution where each mixture element is the prediction of the corresponding model.

Now, very importantly, in continuous state spaces, it doesn't mean that we average together the actual mean state.

We're averaging together the probabilities, which means that if each of these models is Gaussian, and their means are in different places, our output is not one Gaussian with the average of those means.

It's actually multiple different Gaussians.

It's actually a mixture of Gaussians.

So we're mixing the probabilities, not the means.

So when you implement this in homework 4, don't just average together the next states that your models predict.

Actually treat it as a mixture of Gaussians.

Okay, how can we train this bootstrap ensemble to actually get it to represent this parameter posterior?

Well, one mathematical tool we can use is something called the bootstrap.

The main idea in the bootstrap is that we take our single training set, and we generate multiple independent data sets from the single training set to get independent models.

So each model needs to be trained on a data set that is independent from the data set for every other model, but still comes from the same distribution.

Now, if we had a very large amount of data, one very simple way we could do this is we could take our training set and just chop it up into N non-overlapping pieces, and train a different model on each piece.

But that's very wasteful, because we're essentially decimating our data set, and therefore we can't have too many bootstraps, we can't have too many models.

So in the bootstrap ensemble, each of these models is called a bootstrap.

So there's a cool trick that we can do, which is going to maintain our data efficiency and give us as many models as we want.

The idea is to train each θ_i on a data set D_i, which is sampled with replacement from D.

So if D contains N data points, D_i will also contain N data points, but they will be resampled from D with replacement.

Which means that for every entry in D_i, let's say you have N data points in D, you select an integer uniformly at random between 1 and D, and pick the corresponding element from D.

So you select a random entry from 1 to N, pick the element from D, write that into the first entry of D_i.

For the second entry in D_i, pick a random integer between 1 and D, grab it from D, put it in entry 2.

For entry 3, random integer from 1 to N, take it from D, put it in entry 3, and so on and so on and so on.

In expectation, you get a data set that comes from the same distribution as D, but every individual D_i is going to look a little bit different.

Intuitively, you can think of this as putting integer counts on every data point, and those counts can range from 0 to N, although N is very unlikely.

So every model trained on every D_i is going to see a slightly different data set, although statistically the data sets will be similar.

And it turns out this is enough to give you a parameter posterior.

![11_22](images/lec_11_22.png)

So that's the theory.

Now in the world of deep learning, it turns out that training in Bootstrap Ensemble is actually even easier.

So the basic recipe I outlined on the previous slide essentially works.

It's a fairly crude approximation because the number of models we would have is usually small, right?

So if the cost of training one neural net is 3 hours and we have to train 10 of them, that will take 30 hours of compute.

Now you can parallelize it, but it's still expensive.

So usually we'd use a smaller number of models, typically less than 10.

So our uncertainty will be a fairly crude approximation to the true parameter posterior.

Conveniently though, it appears experimentally that if you're training deep neural network models, resampling with replacement is actually usually unnecessary because just the fact that you train your model with stochastic gradient descent and random initialization usually makes the model sufficiently independent even if they are trained on exactly the same data set.

So when implementing this in practice, we can usually actually forego the resampling with replacement.

So that makes things a little easier.

It's important for theoretical results, but practically you can skip it.

